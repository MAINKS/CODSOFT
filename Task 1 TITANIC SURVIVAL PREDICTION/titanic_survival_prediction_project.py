# -*- coding: utf-8 -*-
"""Titanic_Survival_Prediction_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dss6v5qOIETDdkKhLDCV6rBoMuC34HTQ

# Titanic Survival Prediction
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

!pip install streamlit
import streamlit as st

"""# Data Loading and Exploration

## Load the Data:
"""

df = pd.read_csv('Titanic-Dataset.csv')  # Update the path if necessary
df.head()

"""## Explore the Data:

"""

df.info()   # Get an overview of the dataset
df.describe()  # Get statistical summary of the dataset
df.isnull().sum()  # Check for missing values

"""## Visualize survival based on different features:

"""

sns.countplot(x='Survived', data=df)
sns.countplot(x='Pclass', hue='Survived', data=df)
sns.countplot(x='Sex', hue='Survived', data=df)
sns.histplot(df[df['Survived']==1]['Age'], bins=20, kde=False, label='Survived')
sns.histplot(df[df['Survived']==0]['Age'], bins=20, kde=False, label='Not Survived')
plt.legend()

"""# Step 3: Data Preprocessing

## Handle Missing Values:
"""

df['Age'].fillna(df['Age'].median(), inplace=True)

"""##Fill missing embarked with the mode:

"""

df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)

"""##Drop the Cabin column:

"""

df.drop(columns=['Cabin'], inplace=True)

"""#Encode Categorical Variables:

##Convert categorical columns into numerical values:
"""

df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)

"""#Feature Selection: Select features and target variable:

"""

X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']]
y = df['Survived']

"""#Step 4: Model Building and Evaluation : Split the Data:

"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""## Scale the Features"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""## Build the Model:

"""

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

"""## Evaluate the Model

"""

y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')
plt.show()

"""# Download the Trained Model- using Joblib library

"""

import joblib

# Save the trained model
joblib.dump(model, 'titanic_model.pkl')

# Download the model file
from google.colab import files
files.download('titanic_model.pkl')

df.to_csv('processed_titanic_data.csv', index=False)
files.download('processed_titanic_data.csv')